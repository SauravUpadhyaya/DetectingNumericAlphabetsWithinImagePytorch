{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04613cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Process 1: Stepwise\n",
    "\n",
    "##Reference:\n",
    "https://github.com/Digya053/deep-learning-v2-pytorch/blob/master/intro-to-pytorch/Part%203%20-%20Training%20Neural%20Networks%20(Solution).ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9cbd4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import important packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as tf\n",
    "from torch import nn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d53a289",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing helper class\n",
    "import helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "187479bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51267f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform= transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,),)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b760d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download and loading the training data\n",
    "trainset = datasets.MNIST('~/.pytorch/Mnist_data/', download= True, train= True, transform= transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset,batch_size = 64, shuffle= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "001147a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building a feedforward network\n",
    "model = nn.Sequential(\n",
    "                      nn.Linear(784,128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128,64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64,10),\n",
    "                      nn.LogSoftmax(dim=1)\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "591bce28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2960, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Define the loss function\n",
    "criterion= nn.NLLLoss()\n",
    "#Get data\n",
    "images, labels = next(iter(trainloader))\n",
    "#Flatten the images\n",
    "images = images.view(images.shape[0],-1)\n",
    "#Forward pass, get log-probabilities\n",
    "\n",
    "logps = model(images)\n",
    "#Calculate loss with logps and the labels\n",
    "loss = criterion(logps,labels)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74c155df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Backward pass:\n",
      " None\n"
     ]
    }
   ],
   "source": [
    "print('Before Backward pass:\\n', model[0].weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf5f09e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After backward pass:\n",
      " tensor([[ 0.0038,  0.0038,  0.0038,  ...,  0.0038,  0.0038,  0.0038],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0020,  0.0020,  0.0020,  ...,  0.0020,  0.0020,  0.0020],\n",
      "        ...,\n",
      "        [-0.0031, -0.0031, -0.0031,  ..., -0.0031, -0.0031, -0.0031],\n",
      "        [ 0.0002,  0.0002,  0.0002,  ...,  0.0002,  0.0002,  0.0002],\n",
      "        [-0.0016, -0.0016, -0.0016,  ..., -0.0016, -0.0016, -0.0016]])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(\"After backward pass:\\n\", model[0].weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cffe8bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, training the network\n",
    "from torch import optim\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd410b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weight:\n",
      " Parameter containing:\n",
      "tensor([[-0.0304, -0.0285, -0.0292,  ..., -0.0149,  0.0022, -0.0045],\n",
      "        [ 0.0084, -0.0249, -0.0343,  ..., -0.0294, -0.0107,  0.0289],\n",
      "        [-0.0260, -0.0062, -0.0075,  ..., -0.0114,  0.0195,  0.0087],\n",
      "        ...,\n",
      "        [-0.0349, -0.0264, -0.0260,  ..., -0.0190, -0.0304,  0.0241],\n",
      "        [ 0.0341,  0.0247,  0.0047,  ..., -0.0305,  0.0299,  0.0326],\n",
      "        [-0.0083,  0.0060,  0.0224,  ...,  0.0244, -0.0123, -0.0077]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(\"Initial weight:\\n\", model[0].weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f8106cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "        [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "        [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "        ...,\n",
       "        [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "        [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "        [-1., -1., -1.,  ..., -1., -1., -1.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images, labels = next(iter(trainloader)) \n",
    "images.resize_(64,784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "419c6e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient tensor([[ 0.0033,  0.0033,  0.0033,  ...,  0.0033,  0.0033,  0.0033],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0007, -0.0007, -0.0007,  ..., -0.0007, -0.0007, -0.0007],\n",
      "        ...,\n",
      "        [ 0.0015,  0.0015,  0.0015,  ...,  0.0015,  0.0015,  0.0015],\n",
      "        [-0.0023, -0.0023, -0.0023,  ..., -0.0023, -0.0023, -0.0023],\n",
      "        [-0.0002, -0.0002, -0.0002,  ..., -0.0002, -0.0002, -0.0002]])\n"
     ]
    }
   ],
   "source": [
    "#When you do multiple backwards passes with the same parameters, the gradients are accumulated. \n",
    "#This means that you need to zero the gradients on each training pass or you'll retain gradients from previous \n",
    "#training batches.\n",
    "\n",
    "optimizer.zero_grad()\n",
    "output = model(images)\n",
    "loss = criterion(output, labels)\n",
    "loss.backward()\n",
    "print(\"Gradient\", model[0].weight.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b834cb16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated weights -  Parameter containing:\n",
      "tensor([[-0.0304, -0.0285, -0.0293,  ..., -0.0149,  0.0022, -0.0045],\n",
      "        [ 0.0084, -0.0249, -0.0343,  ..., -0.0294, -0.0107,  0.0289],\n",
      "        [-0.0260, -0.0062, -0.0075,  ..., -0.0114,  0.0195,  0.0087],\n",
      "        ...,\n",
      "        [-0.0350, -0.0264, -0.0260,  ..., -0.0190, -0.0304,  0.0241],\n",
      "        [ 0.0341,  0.0247,  0.0047,  ..., -0.0305,  0.0299,  0.0326],\n",
      "        [-0.0083,  0.0060,  0.0224,  ...,  0.0244, -0.0123, -0.0077]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "## Take an update step and few the new weights\n",
    "optimizer.step()\n",
    "print(\"Updated weights - \", model[0].weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4727ec01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.6316079390423892\n",
      "Training loss: 0.277895759830851\n",
      "Training loss: 0.21369454951317451\n",
      "Training loss: 0.17226239126215356\n",
      "Training loss: 0.14411356424821464\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "##Summarizing above one \n",
    "#training for real\n",
    "#mplement the training pass for our network. If you implemented it correctly, you should see the training \n",
    "#loss drop with each epoch.\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as tf\n",
    "from torch import nn\n",
    "#Importing helper class\n",
    "import helper\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "transform= transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,),)])\n",
    "\n",
    "#Download and loading the training data\n",
    "trainset = datasets.MNIST('~/.pytorch/Mnist_data/', download= True, train= True, transform= transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset,batch_size = 64, shuffle= True)\n",
    "\n",
    "\n",
    "#Building a feedforward network\n",
    "model = nn.Sequential(\n",
    "                      nn.Linear(784,128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128,64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64,10),\n",
    "                      nn.LogSoftmax(dim=1)\n",
    "                     )\n",
    "#Define the loss function\n",
    "criterion= nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.03)\n",
    "epochs = 5\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        images = images.view(images.shape[0], -1)\n",
    "    \n",
    "        # TODO: Training pass\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(f\"Training loss: {running_loss/len(trainloader)}\")\n",
    "        \n",
    "        \n",
    "%matplotlib inline\n",
    "import helper\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "\n",
    "img = images[0].view(1, 784)\n",
    "# Turn off gradients to speed up this part\n",
    "with torch.no_grad():\n",
    "    logps = model(img)\n",
    "\n",
    "# Output of the network are log-probabilities, need to take exponential for probabilities\n",
    "ps = torch.exp(logps)\n",
    "helper.view_classify(img.view(1, 28, 28), ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac2f555",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d253e472",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
